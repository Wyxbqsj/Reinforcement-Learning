{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \n",
      "Q-table:\n",
      "\n",
      "       left     right\n",
      "0  0.000000  0.004320\n",
      "1  0.000000  0.025005\n",
      "2  0.000030  0.111241\n",
      "3  0.000000  0.368750\n",
      "4  0.027621  0.745813\n",
      "5  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "\n",
    "np.random.seed(2) #reproducible 计算机产生一组伪随机数列，虽然是随机的，但每次运行的随机过程都是一样的\n",
    "\n",
    "#创建几个lobal variables\n",
    "N_STATES=6 #the length of the 1 dimensional world\n",
    "ACTIONS=['left','right'] #两个可选动作\n",
    "EPSILON=0.9 #ϵ - greedy中ϵ=0.9\n",
    "ALPHA=0.1 #学习率\n",
    "LAMBDA=0.9 #折扣系数\n",
    "MAX_EPISODES=13 #只玩13回合\n",
    "FRESH_TIME=0.03 #用time模块规定走一步花0.3秒\n",
    "\n",
    "#建立Q表\n",
    "def build_q_table(n_states,actions):#用pandas创建一个表格\n",
    "    table=pd.DataFrame(\n",
    "        np.zeros((n_states,len(actions))), #全0初始化\n",
    "        columns=actions\n",
    "    )\n",
    "    return table\n",
    "#build_q_table(N_STATES,ACTIONS) #行名是s,列名是a\n",
    "\n",
    "\n",
    "\n",
    "#创建选动作的功能 This is how to choose an action\n",
    "def choose_action(state,q_table): #根据目前所处的状态和Q表\n",
    "    state_actions=q_table.iloc[state,:]\n",
    "    if(np.random.uniform()>EPSILON) or ((state_actions==0).all()):\n",
    "        action_name=np.random.choice(ACTIONS) #随机数大于0.9（10%的情况）或处于初始状态就随机选择ACTIONS里面的动作\n",
    "    else:\n",
    "        action_name=state_actions.idxmax() #随机数小于0.9（90%的情况）选择当前状态下动作估值大的动作\n",
    "        # replace argmax to idxmax as argmax means a different function in newer version of pandas\n",
    "    return action_name\n",
    "\n",
    "\n",
    "#This is how agent will interact with the environment\n",
    "def get_env_feedback(S,A): #现在的状态S，采取动作A，获得环境给的奖励值R，到达下一个状态S_\n",
    "    if A=='right':\n",
    "        if S==N_STATES-2:\n",
    "            S_='terminal'\n",
    "            R=1\n",
    "        else:\n",
    "            S_=S+1\n",
    "            R=0\n",
    "    else:\n",
    "        R=0\n",
    "        if S==0:\n",
    "            S_=S\n",
    "        else:\n",
    "            S_=S-1\n",
    "    return S_,R\n",
    "\n",
    "#创建环境，怎么让探索者在这样一个一维环境中移动寻宝\n",
    "def update_env(S, episode, step_counter):\n",
    "    # This is how environment be updated\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(FRESH_TIME)\n",
    "        \n",
    "#主循环 Main part of RL loop\n",
    "def rl():\n",
    "    q_table=build_q_table(N_STATES,ACTIONS)\n",
    "    for episode in range(MAX_EPISODES): #从第一个回合到最后一个回合\n",
    "        step_counter=0\n",
    "        S=0 #初始情况把探索者放在最左边\n",
    "        is_terminated=False #初始不是终止符(回合结束)\n",
    "        update_env(S,episode,step_counter) #第一步更新一下环境\n",
    "        while not is_terminated: #回合没有结束\n",
    "            \n",
    "            A=choose_action(S,q_table) #根据初始的state选择一个action\n",
    "            S_,R=get_env_feedback(S,A) #根据state和action在环境中得到下一个state和reward\n",
    "            q_predict=q_table.loc[S,A] #估计值\n",
    "            if S_!='terminal':#探索者还没有寻到宝物（未到达终点）\n",
    "                q_target=R+LAMBDA*q_table.iloc[S_,:].max() #真实值\n",
    "            else:#探索者寻到宝物到达终点，没有下一步的状态，此时真实值就是reward\n",
    "                q_target=R\n",
    "                is_terminated=True #该episode结束\n",
    "            \n",
    "            q_table.loc[S,A]+=ALPHA*(q_target-q_predict)\n",
    "            S=S_ #移向下一个状态\n",
    "            \n",
    "            update_env(S,episode,step_counter+1) #探索者走了每一步，再更新一下环境\n",
    "            step_counter+=1\n",
    "    return q_table\n",
    "\n",
    "\n",
    "    \n",
    "#运行\n",
    "if __name__=='__main__':\n",
    "    q_table=rl() #最终的q_table(学习好的Q表)\n",
    "    print('\\r\\nQ-table:\\n')#打印\n",
    "    print(q_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
